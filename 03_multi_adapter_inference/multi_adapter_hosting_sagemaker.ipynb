{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844f22ae-6b01-4a17-820b-83f9d34de23e",
   "metadata": {},
   "source": [
    "# Serve multiple LoRA adapters efficiently on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cd38c-ad40-4a3e-b225-4d1f707a0c0e",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to serve many Low-Rank Adapters (LoRA) on top of the same base model efficiently on the same GPU. In order to do this, we'll deploy the LoRA Exchange ([LoRAX](https://github.com/predibase/lorax/tree/main)) inference server to SageMaker Hosting. \n",
    "\n",
    "These are the steps we will take:\n",
    "\n",
    "1. [Setup our environment](#setup)\n",
    "2. [Build a new LoRAX container image compatible with SageMaker, push it to Amazon ECR](#container)\n",
    "3. [Download adapters from the HuggingFace Hub and upload them to S3](#download_adapter)\n",
    "4. [Deploy the extended LoRAX container to SageMaker](#deploy)\n",
    "5. [Compare outputs of the base model and the adapter model](#compare)\n",
    "6. [Benchmark our deployed endpoint under different traffic patterns - same adapter, and random access to many adapters](#benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65e8be-3073-4436-b311-11a3de09e101",
   "metadata": {},
   "source": [
    "## What is LoRAX? \n",
    "\n",
    "LoRAX is a production-ready framework specialized in multi-adapter serving that  efficiently share the same GPU resources, which dramatically reduces the cost of serving without compromising on throughput or latency. Some of the features that enable this are: \n",
    "\n",
    "* Dynamic Adapter Loading - fine-tuned LoRA weights are loaded from storage (local or remote) just-in-time as requests come in at runtime\n",
    "* Tiered Weight Caching - fast exchanging of LoRA adapters between requests, and offloading of adapter weights to CPU and disk as they are not needed to avoid out-of-memory errors.\n",
    "* Continuous Multi-Adapter Batching - a fair scheduling policy that continuously batches requests targeted at different LoRA adapters so they can be processed in paralle, optimizing aggregate throughput.\n",
    "* Optimized Inference - high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels ([flash-attention](https://arxiv.org/abs/2307.08691), [paged attention](https://arxiv.org/abs/2309.06180), [SGMV](https://arxiv.org/abs/2310.18547)), quantization, token streaming.\n",
    "\n",
    "You can read more about LoRAX [here](https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4372522-fb5d-4cad-be37-d4f21f794698",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf3acc-e2fc-48ca-a21e-4377c7638d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8394b-5d69-4c02-97fe-d00eccc4da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess._region_name \n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d6fd3-5d59-4bc2-8cf2-ad4c32d072b7",
   "metadata": {},
   "source": [
    "<a id=\"container\"></a>\n",
    "## 2. Build a new LoRAX container image compatible with SageMaker, push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9d085-897f-46a4-875f-798f0ebb1f61",
   "metadata": {},
   "source": [
    "This example includes a `Dockerfile` and `sagemaker_entrypoint.sh` in the `sagemaker_lorax` directory. Building this new container image makes LoRAX compatible with SageMaker Hosting, namely launching the server on port 8080 via the container's `ENTRYPOINT` instruction. [Here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) you can find the basic interfaces required to adapt any container for deployment on Sagemaker Hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0eafe8-2398-43e0-9dd6-e211c23ba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat sagemaker_lorax/sagemaker_entrypoint.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cc01a-13e6-4d6e-9637-00dc2e94050d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat sagemaker_lorax/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0c937-9e17-42ea-afc5-31d5dbff6331",
   "metadata": {},
   "source": [
    "We build the new container image and push it to a new ECR repository. Note SageMaker [supports private Docker registries](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02779726-b57f-472b-9d09-6a1aabc56a10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s {region}\n",
    "algorithm_name=\"sagemaker-lorax\"  # name of your algorithm\n",
    "tag=\"0.8.0\"\n",
    "region=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "image_uri=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" --region $region > /dev/null\n",
    "fi\n",
    "\n",
    "cd sagemaker_lorax/ && docker build  --build-arg VERSION=$tag -t ${algorithm_name}:${tag} .\n",
    "\n",
    "# Authenticate Docker to an Amazon ECR registry\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Tag the image\n",
    "docker tag ${algorithm_name}:${tag} ${image_uri}\n",
    "\n",
    "# Push the image to the repository\n",
    "docker push ${image_uri}\n",
    "\n",
    "# Save image name to tmp file to use when deploying endpoint\n",
    "echo $image_uri > /tmp/image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d38628-9317-408f-906d-e5c9ce80fbb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"download_adapter\"></a>\n",
    "## 3. Download adapter from HuggingFace Hub and push it to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cb518-220c-491f-ae30-50f14f2b4ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to simulate storing our adapter weights on S3, and having LoRAX load them dynamically as we invoke them. This enables most scenarios, including deployment after you’ve finetuned your own adapter and pushed it to S3, as well as securing deployments with no internet access inside your VPC, as detailed in this [blog post](https://www.philschmid.de/sagemaker-llm-vpc#2-upload-the-model-to-amazon-s3).\n",
    "\n",
    "We first download an adapter trained with Mistral Instruct v0.1 as the base model to a local directory. This particular adapter was trained on GSM8K, a grade school math dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3230c3b-7740-4727-b8a1-eda109afe077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n",
    "# create model dir\n",
    "model_dir = Path('mistral-adapter')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_dir), # download to model dir\n",
    "    local_dir_use_symlinks=False, # use no symlinks to save disk space\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b00f3-ca40-4b87-9456-71df3e6efe95",
   "metadata": {},
   "source": [
    "We copy this same adapter `n_adapters` times to different S3 prefixes in our SageMaker session bucket, simulating a large number of adapters we want to serve on the same endpoint and underlying GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a490c-a540-4e0d-8a44-a8b886bbfac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_folder_to_s3(local_path, s3_bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_object_key = os.path.join(s3_prefix, os.path.relpath(local_file_path, local_path))\n",
    "            s3.upload_file(local_file_path, s3_bucket, s3_object_key)\n",
    "\n",
    "# Upload the folder n_adapters times under different prefixes\n",
    "n_adapters=50\n",
    "base_prefix = 'lorax/mistral-adapters'\n",
    "for i in range(1, n_adapters+1):\n",
    "    prefix = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "\n",
    "    upload_folder_to_s3(model_dir, sagemaker_session_bucket, prefix)\n",
    "    print(f'Uploaded folder to S3 with prefix: {prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ed1b2-8b7c-4e9b-bace-7ace5f78e1ca",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## 4. Deploy SageMaker endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24eae-766a-4ec7-8ecf-de45f1092051",
   "metadata": {},
   "source": [
    "Now we deploy a SageMaker endpoint, pointing to our SageMaker session bucket as the ADAPTER_BUCKET env variable, which enables downloading adapters from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731f030-39ad-4dd7-854e-59d1e0c3fe0c",
   "metadata": {},
   "source": [
    "If you have any problems deploying on g5.xlarge, you can change the instance type to g5.2xlarge or g5.4xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02925d2b-147c-4c84-af40-cc7c32fa7993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "from sagemaker import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Retrieve image_uri from tmp file\n",
    "image_uri = !cat /tmp/image_uri\n",
    "# Increased health check timeout to give time for model download\n",
    "health_check_timeout = 800\n",
    "number_of_gpu = 1\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = endpoint_name = sagemaker.utils.name_from_base(\"sm-lorax\")\n",
    "\n",
    "\n",
    "# Model and Endpoint configuration parameters\n",
    "env = {\n",
    "  'HF_MODEL_ID': \"mistralai/Mistral-7B-Instruct-v0.1\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'ADAPTER_BUCKET': sagemaker_session_bucket,\n",
    "}\n",
    "\n",
    "lorax_model = Model(\n",
    "    image_uri=image_uri[0],\n",
    "    role=role,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "lorax_predictor = lorax_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout, \n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c9e7a-8a1e-4b02-bc60-77f87f7aa5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can reinstantiate the Predictor object if you restart the notebook or Predictor is None\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "endpoint_name = endpoint_name\n",
    "\n",
    "lorax_predictor = Predictor(\n",
    "    endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308c12e-365c-4737-90f5-998f849beffe",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "## 5. Invoke base model and adapter, compare outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ae3e2-8634-4e4d-957a-a3d81d7490e7",
   "metadata": {},
   "source": [
    "We can invoke the base Mistral model, as well as any of the adapters in our bucket! LoRAX will take care of downloading them, continuously batch requests for different adapters, and manage DRAM and RAM by loading/offloading adapters.\n",
    "\n",
    "Let’s inspect the difference between the base model’s response and the adapter’s response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ea512-e0af-40c5-ba3e-1253eedfd011",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ I observed a weird error that I haven't debugged yet, where S3 download failed for adapters ID 1 through 5, but worked as expected for all other adapters. Something with the S3 prefix. Added 0 before adapter id if id < 10 as a workaround.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167babae-0fb9-4a7f-aba9-fe4e266ebdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "payload_base = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "    }\n",
    "}\n",
    "\n",
    "payload_adapter = {\n",
    "\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"adapter_id\": f'{base_prefix}/01',\n",
    "        # \"adapter_id\" : adapter_uri, \n",
    "        \"adapter_source\": \"s3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response_base = lorax_predictor.predict(payload_base)\n",
    "response_adapter = lorax_predictor.predict(payload_adapter)\n",
    "\n",
    "print(f'Base model output:\\n-------------\\n {response_base[0][\"generated_text\"]}')\n",
    "print(f'Adapter output:\\n-------------\\n {response_adapter[0][\"generated_text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010056-9b46-4c62-880c-9f4552aa2af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## 6. Benchmark single adapter vs. random access to adapters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67667c2d-c154-42dc-9be7-d577f8855e1d",
   "metadata": {},
   "source": [
    "First, we individually call each of the adapters in sequence, to make sure they are previously downloaded to the endpoint instance’s disk. We want to exclude S3 download latency from the benchmark metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeea08-31e8-411c-a57b-c4acdd702f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1,n_adapters+1)):\n",
    "    adapter_id = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "    payload_adapter = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"adapter_id\": adapter_id,\n",
    "        \"adapter_source\": \"s3\"\n",
    "        }\n",
    "    }\n",
    "    lorax_predictor.predict(payload_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eeb399-7490-4697-9871-777038232200",
   "metadata": {},
   "source": [
    "Now we are ready to benchmark. For the single adapter case, we invoke the adapter `total_requests` times from `num_threads` concurrent clients.\n",
    "\n",
    "For the multi-adapter case, we invoke a random adapter from any of the clients, until all adapters have been invoked `total_requests//num_adapters` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520c962-f064-4e8a-a9ce-2405084e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if you run into connection pool errors\n",
    "# import botocore\n",
    "\n",
    "# Configure botocore to use a larger connection pool\n",
    "# config = botocore.config.Config(max_pool_connections=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94ddaa-4d57-4139-8e76-b87593fb155e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Configuration\n",
    "total_requests = 300\n",
    "num_adapters = 50\n",
    "num_threads = 20  # Adjust based on your system capabilities\n",
    "\n",
    "\n",
    "# Shared lock and counters for # invocations of each adapter \n",
    "adapter_counters = [total_requests // num_adapters] * num_adapters\n",
    "counters_lock = threading.Lock()\n",
    "\n",
    "def invoke_adapter(aggregate_latency, single_adapter=False):\n",
    "    global total_requests\n",
    "    latencies = []\n",
    "    while True:\n",
    "        with counters_lock:\n",
    "            if single_adapter:\n",
    "                adapter_id = 1\n",
    "                if total_requests > 0:\n",
    "                    total_requests -= 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                # Find an adapter that still needs to be called\n",
    "                remaining_adapters = [i for i, count in enumerate(adapter_counters) if count > 0]\n",
    "                if not remaining_adapters:\n",
    "                    break\n",
    "                adapter_id = random.choice(remaining_adapters) + 1\n",
    "                adapter_counters[adapter_id - 1] -= 1\n",
    "\n",
    "        prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "        invoke_adapter_id = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "        payload_adapter = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 64,\n",
    "                \"adapter_id\": invoke_adapter_id,\n",
    "                \"adapter_source\": \"s3\"\n",
    "            }\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        response_adapter = lorax_predictor.predict(payload_adapter)\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    aggregate_latency.extend(latencies)\n",
    "\n",
    "def benchmark_scenario(single_adapter=False):\n",
    "    threads = []\n",
    "    all_latencies = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread_latencies = []\n",
    "        all_latencies.append(thread_latencies)\n",
    "        thread = threading.Thread(target=invoke_adapter, args=(thread_latencies, single_adapter))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    total_latency = sum([sum(latencies) for latencies in all_latencies])\n",
    "    total_requests_made = sum([len(latencies) for latencies in all_latencies])\n",
    "    average_latency = total_latency / total_requests_made\n",
    "    throughput = total_requests_made / (time.time() - start_time)\n",
    "\n",
    "    print(f\"Total Time: {time.time() - start_time}s\")\n",
    "    print(f\"Average Latency: {average_latency} s\")\n",
    "    print(f\"Throughput: {throughput} requests/s\")\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Benchmarking: Single Adapter Multiple Times\")\n",
    "benchmark_scenario(single_adapter=True)\n",
    "\n",
    "print(\"\\nBenchmarking: Multiple Adapters with Random Access\")\n",
    "benchmark_scenario()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9521-f5ed-487b-ba84-5a834add3d78",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## 7. Cleanup endpoint resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920da7-c0ce-4f3f-9eea-297abb49b342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lorax_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
